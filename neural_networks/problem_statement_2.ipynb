{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "545bbd63",
   "metadata": {},
   "source": [
    "# The Problem: What countries will have increased listening time based on the amount of user engagement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "87467345",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "93c72072",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>minutes_streamed_per_day</th>\n",
       "      <th>repeat_song_rate</th>\n",
       "      <th>discover_weekly_engagement</th>\n",
       "      <th>number_of_songs_liked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>34</td>\n",
       "      <td>295</td>\n",
       "      <td>16.74</td>\n",
       "      <td>47.42</td>\n",
       "      <td>138</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  minutes_streamed_per_day  repeat_song_rate  \\\n",
       "0   34                       295             16.74   \n",
       "\n",
       "   discover_weekly_engagement  number_of_songs_liked  \n",
       "0                       47.42                    138  "
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "music_data = pd.read_csv('../data/clean_data/country_listen_time.csv')\n",
    "music_data.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2498c863",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "3148fb93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset size: 5000 individual users\n",
      "Input features: 4 numerical features\n",
      "Target: Individual listening minutes\n",
      "\n",
      "Feature statistics:\n",
      "age:\n",
      " Range: 13.0 = 60.0\n",
      " Mean: 36.7\n",
      " Standard Deviation: 13.8\n",
      "repeat_song_rate:\n",
      " Range: 5.0 = 80.0\n",
      " Mean: 42.4\n",
      " Standard Deviation: 21.4\n",
      "discover_weekly_engagement:\n",
      " Range: 10.0 = 90.0\n",
      " Mean: 50.3\n",
      " Standard Deviation: 23.2\n",
      "number_of_songs_liked:\n",
      " Range: 1.0 = 500.0\n",
      " Mean: 253.5\n",
      " Standard Deviation: 146.4\n",
      "\n",
      "Target Statistics\n",
      "Minutes Streamed Per Day\n",
      " Range: 10.000000 - 600.0\n",
      " Mean: 309.2\n",
      " Standard Deviation 172.0\n"
     ]
    }
   ],
   "source": [
    "# Extract features and target\n",
    "features = ['age','repeat_song_rate','discover_weekly_engagement','number_of_songs_liked']\n",
    "target_col = ['minutes_streamed_per_day']\n",
    "\n",
    "x_raw = music_data[features].values\n",
    "y_raw = music_data[target_col].values.reshape(-1,1)\n",
    "\n",
    "print(f\"\\nDataset size: {len(music_data)} individual users\")\n",
    "print(f\"Input features: {len(features)} numerical features\")\n",
    "print(f\"Target: Individual listening minutes\")\n",
    "\n",
    "print(f\"\\nFeature statistics:\")\n",
    "for i, feature in enumerate(features):\n",
    "    values = x_raw[:, i]\n",
    "    print(f\"{feature}:\")\n",
    "    print(f\" Range: {values.min():.1f} = {values.max():.1f}\")\n",
    "    print(f\" Mean: {values.mean():.1f}\")\n",
    "    print(f\" Standard Deviation: {values.std():.1f}\")\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "How target statistics will help to verify if we are getting correct numbers\n",
    "\n",
    "1. Range (Min - Max): Data Spread Analysis\n",
    "    * Scale awareness: 600 minutes = 10 hours (huge daily listening!)\n",
    "    * Outlier detection: Are there unrealistic values?\n",
    "    * Network design: Large ranges need careful normalization\n",
    "    * Activation choice: ReLU works well with positive ranges\n",
    "\n",
    "2. Mean: Central Tendency\n",
    "\n",
    "    * Baseline prediction: Random guessing would predict the mean\n",
    "    * Normalization center: StandardScaler centers data around 0\n",
    "    * Reality check: Does average make sense? (5 hours/day is reasonable)\n",
    "    * R2 calculation: R2 compares predictions to mean baseline\n",
    "\n",
    "3. Standard Deviation: Variability\n",
    "    * Learning difficulty: High std = more patterns to learn\n",
    "    * Normalization scaling: StandardScaler uses std for scaling\n",
    "    * Prediction quality: Good models should predict within 1-2 std of actual\n",
    "    * Feature importance: Target variation drives what features matter    \n",
    "\"\"\"\n",
    "\n",
    "print(f\"\\nTarget Statistics\")\n",
    "print(f\"Minutes Streamed Per Day\")\n",
    "print(f\" Range: {y_raw.min():1f} - {y_raw.max():.1f}\")   # Data spread\n",
    "print(f\" Mean: {y_raw.mean():.1f}\")                      # Central tendency\n",
    "print(f\" Standard Deviation {y_raw.std():.1f}\")          # Variability\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "3ba45a48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "After normalization\n",
      "Features - Mean: 0.000, Standard Deviation: 1.000\n",
      "Target - Mean: 0.000, Standard Deviation: 1.000\n",
      "\n",
      "Data Splits:\n",
      "Training set: 3000 samples\n",
      "Validation set: 1000 samples\n",
      "Test set: 1000 samples\n"
     ]
    }
   ],
   "source": [
    "# Normalize all features\n",
    "scaler_features = StandardScaler()\n",
    "x = scaler_features.fit_transform(x_raw)\n",
    "\n",
    "# Normalize target values\n",
    "scaler_target = StandardScaler()\n",
    "y = scaler_target.fit_transform(y_raw)\n",
    "\n",
    "print(f\"\\nAfter normalization\")\n",
    "print(f\"Features - Mean: {x.mean():.3f}, Standard Deviation: {x.std():.3f}\")\n",
    "print(f\"Target - Mean: {y.mean():.3f}, Standard Deviation: {y.std():.3f}\")\n",
    "\n",
    "# Split data into train/val/test (60%/20%/20%)\n",
    "x_train, x_temp, y_train, y_temp = train_test_split(x,y, test_size=0.4, random_state=42)\n",
    "x_val,x_test,y_val,y_test = train_test_split(x_temp,y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "print(f\"\\nData Splits:\")\n",
    "print(f\"Training set: {x_train.shape[0]} samples\")\n",
    "print(f\"Validation set: {x_val.shape[0]} samples\")\n",
    "print(f\"Test set: {x_test.shape[0]} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c7aeb34",
   "metadata": {},
   "source": [
    "## Neural Network Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "2401b933",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegressionNeuralNetwork:\n",
    "    \"\"\"\n",
    "    2-layer Neural Network specialized for Regression tasks\n",
    "    - Continuous output with linear activation\n",
    "    - Mean Squared Error loss function\n",
    "    - R² score evaluation metric\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, output_size=1):\n",
    "        \"\"\"\n",
    "        Initialize regression neural network\n",
    "        \n",
    "        Args:\n",
    "            input_size: Number of input features\n",
    "            hidden_size: Number of hidden layer neurons\n",
    "            output_size: Number of continuous outputs (usually 1)\n",
    "        \"\"\"\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        # Xavier initialization\n",
    "        self.W1 = np.random.randn(input_size, hidden_size) / np.sqrt(input_size)\n",
    "        self.b1 = np.zeros((1, hidden_size))\n",
    "        \n",
    "        self.W2 = np.random.randn(hidden_size, output_size) / np.sqrt(hidden_size)\n",
    "        self.b2 = np.zeros((1, output_size))\n",
    "        \n",
    "    def relu(self, x):\n",
    "        \"\"\"ReLU activation: f(x) = max(0, x)\"\"\"\n",
    "        return np.maximum(0, x)\n",
    "    \n",
    "    def relu_derivative(self, x):\n",
    "        \"\"\"ReLU derivative: f'(x) = 1 if x > 0, else 0\"\"\"\n",
    "        return (x > 0).astype(float)\n",
    "    \n",
    "    def forward_pass(self, X):\n",
    "        \"\"\"\n",
    "        Forward propagation for regression:\n",
    "        z1 = X @ W1 + b1\n",
    "        a1 = ReLU(z1)\n",
    "        z2 = a1 @ W2 + b2  (Linear output, no activation)\n",
    "        \"\"\"\n",
    "        # Hidden layer\n",
    "        self.z1 = X @ self.W1 + self.b1\n",
    "        self.a1 = self.relu(self.z1)\n",
    "        \n",
    "        # Output layer (linear for regression)\n",
    "        self.z2 = self.a1 @ self.W2 + self.b2\n",
    "        \n",
    "        return self.z2\n",
    "    \n",
    "    def mse_loss(self, y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Mean Squared Error loss for regression:\n",
    "        L = Σ(y_true - y_pred)² / (2m)\n",
    "        \"\"\"\n",
    "        m = y_true.shape[0]\n",
    "        mse = np.sum((y_true - y_pred) ** 2) / (2 * m)\n",
    "        return mse\n",
    "    \n",
    "    def backward_pass(self, X, y_true):\n",
    "        \"\"\"\n",
    "        Backpropagation for regression:\n",
    "        \n",
    "        Output layer (MSE derivative):\n",
    "        dL/dz2 = (y_pred - y_true) / m\n",
    "        \n",
    "        Hidden layer:\n",
    "        dL/da1 = dL/dz2 @ W2^T\n",
    "        dL/dz1 = dL/da1 * ReLU'(z1)\n",
    "        \"\"\"\n",
    "        m = X.shape[0]\n",
    "        \n",
    "        # Output layer gradients (MSE derivative)\n",
    "        dz2 = (self.z2 - y_true) / m\n",
    "        dW2 = self.a1.T @ dz2\n",
    "        db2 = np.sum(dz2, axis=0, keepdims=True)\n",
    "        \n",
    "        # Hidden layer gradients\n",
    "        da1 = dz2 @ self.W2.T\n",
    "        dz1 = da1 * self.relu_derivative(self.z1)\n",
    "        dW1 = X.T @ dz1\n",
    "        db1 = np.sum(dz1, axis=0, keepdims=True)\n",
    "        \n",
    "        return dW1, db1, dW2, db2\n",
    "    \n",
    "    def train(self, X_train, y_train, X_val, y_val, epochs=1000, learning_rate=0.01, early_stopping_patience=50):\n",
    "        \"\"\"Train regression neural network with validation monitoring\"\"\"\n",
    "        train_losses = []\n",
    "        val_losses = []\n",
    "        val_r2_scores = []\n",
    "        \n",
    "        best_val_loss = float('inf')\n",
    "        patience_counter = 0\n",
    "        best_weights = None\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            # Forward pass and training loss\n",
    "            y_pred_train = self.forward_pass(X_train)\n",
    "            train_loss = self.mse_loss(y_train, y_pred_train)\n",
    "            train_losses.append(train_loss)\n",
    "            \n",
    "            # Backward pass and weight updates\n",
    "            dW1, db1, dW2, db2 = self.backward_pass(X_train, y_train)\n",
    "            \n",
    "            self.W1 -= learning_rate * dW1\n",
    "            self.b1 -= learning_rate * db1\n",
    "            self.W2 -= learning_rate * dW2\n",
    "            self.b2 -= learning_rate * db2\n",
    "            \n",
    "            # Validation evaluation\n",
    "            y_pred_val = self.forward_pass(X_val)\n",
    "            val_loss = self.mse_loss(y_val, y_pred_val)\n",
    "            val_losses.append(val_loss)\n",
    "            \n",
    "            val_r2 = self.r2_score(X_val, y_val)\n",
    "            val_r2_scores.append(val_r2)\n",
    "            \n",
    "            # Early stopping logic\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                patience_counter = 0\n",
    "                best_weights = {\n",
    "                    'W1': self.W1.copy(), 'b1': self.b1.copy(),\n",
    "                    'W2': self.W2.copy(), 'b2': self.b2.copy()\n",
    "                }\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "            \n",
    "            if epoch % 200 == 0:\n",
    "                print(f\"Epoch {epoch} | Train Loss: {train_loss:.6f} | Val Loss: {val_loss:.6f} | Val R²: {val_r2:.4f}\")\n",
    "            \n",
    "            if patience_counter >= early_stopping_patience:\n",
    "                print(f\"Early stopping at epoch {epoch}. Best validation loss: {best_val_loss:.6f}\")\n",
    "                self.W1, self.b1 = best_weights['W1'], best_weights['b1']\n",
    "                self.W2, self.b2 = best_weights['W2'], best_weights['b2']\n",
    "                break\n",
    "        \n",
    "        return {\n",
    "            'train_losses': train_losses,\n",
    "            'val_losses': val_losses,\n",
    "            'val_r2_scores': val_r2_scores\n",
    "        }\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Get continuous value predictions\"\"\"\n",
    "        return self.forward_pass(X)\n",
    "    \n",
    "    def mse(self, X, y_true):\n",
    "        \"\"\"Calculate Mean Squared Error\"\"\"\n",
    "        y_pred = self.predict(X)\n",
    "        return np.mean((y_true - y_pred) ** 2)\n",
    "    \n",
    "    def r2_score(self, X, y_true):\n",
    "        \"\"\"\n",
    "        Calculate R² score (coefficient of determination)\n",
    "        R² = 1 - (SS_res / SS_tot)\n",
    "        \"\"\"\n",
    "        y_pred = self.predict(X)\n",
    "        ss_res = np.sum((y_true - y_pred) ** 2)\n",
    "        ss_tot = np.sum((y_true - np.mean(y_true)) ** 2)\n",
    "        \n",
    "        # Handle edge case where all predictions are the same\n",
    "        if ss_tot == 0:\n",
    "            return 0.0\n",
    "            \n",
    "        return 1 - (ss_res / ss_tot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c59cfb7",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "f1690a74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Architecture: 4 -> 12 -> 1\n",
      "Input features: Age + 3 engagement metrics = 4 total\n",
      "Data points: 5000\n"
     ]
    }
   ],
   "source": [
    "# network architecture for multi-feature input\n",
    "input_size = x.shape[1]  # 13 (10 countries + 3 engagement metrics)\n",
    "hidden_size = 12          # Hidden layer neurons \n",
    "output_size = 1          # 1 (total minutes)\n",
    "\n",
    "print(f\"Architecture: {input_size} -> {hidden_size} -> {output_size}\")\n",
    "print(f\"Input features: Age + 3 engagement metrics = 4 total\")\n",
    "print(f\"Data points: {len(x)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "570a26be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 | Train Loss: 0.701199 | Val Loss: 0.715622 | Val R²: -0.4080\n",
      "Epoch 200 | Train Loss: 0.508063 | Val Loss: 0.518282 | Val R²: -0.0197\n",
      "Epoch 400 | Train Loss: 0.500898 | Val Loss: 0.512874 | Val R²: -0.0091\n",
      "Epoch 600 | Train Loss: 0.498530 | Val Loss: 0.511964 | Val R²: -0.0073\n",
      "Epoch 800 | Train Loss: 0.497403 | Val Loss: 0.511804 | Val R²: -0.0070\n",
      "Epoch 1000 | Train Loss: 0.496743 | Val Loss: 0.511773 | Val R²: -0.0069\n",
      "Epoch 1200 | Train Loss: 0.496295 | Val Loss: 0.511753 | Val R²: -0.0068\n",
      "Epoch 1400 | Train Loss: 0.495957 | Val Loss: 0.511744 | Val R²: -0.0068\n",
      "Epoch 1600 | Train Loss: 0.495701 | Val Loss: 0.511717 | Val R²: -0.0068\n",
      "Epoch 1800 | Train Loss: 0.495503 | Val Loss: 0.511671 | Val R²: -0.0067\n",
      "\n",
      "Final Result\n",
      "Training MSE: 0.990669 | R2 0.0027\n",
      "Validation MSE: 1.023246 | R2: -0.0066\n",
      "Test MSE: 1.004485 | R2: -0.0020\n"
     ]
    }
   ],
   "source": [
    "# Initialize network \n",
    "nn = RegressionNeuralNetwork(input_size, hidden_size, output_size)\n",
    "\n",
    "# Train the network\n",
    "training_history = nn.train(x_train, y_train, x_val,y_val, epochs=2000, learning_rate=0.01, early_stopping_patience=100)\n",
    "\n",
    "# Evaluate performance on all three sets\n",
    "train_mse = nn.mse(x_train, y_train)\n",
    "val_mse = nn.mse(x_val,y_val)\n",
    "test_mse = nn.mse(x_test, y_test)\n",
    "\n",
    "train_r2 = nn.r2_score(x_train, y_train)\n",
    "val_r2 = nn.r2_score(x_val,y_val)\n",
    "test_r2 = nn.r2_score(x_test,y_test)\n",
    "\n",
    "print(f\"\\nFinal Result\")\n",
    "print(f\"Training MSE: {train_mse:.6f} | R2 {train_r2:.4f}\")\n",
    "print(f\"Validation MSE: {val_mse:.6f} | R2: {val_r2:.4f}\")\n",
    "print(f\"Test MSE: {test_mse:.6f} | R2: {test_r2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a60082",
   "metadata": {},
   "source": [
    "According to these results, the model learned almost nothing. However, there are some signs of learning as shown below:\n",
    "\n",
    "- Epoch 0:   Val R²: -0.41 (terrible)\n",
    "- Epoch 200: Val R²: -0.02 (almost zero)  \n",
    "- Epoch 1800: Val R²: -0.007 (basically zero)\n",
    "\n",
    "There is a trend, but it plateaued at near zero.\n",
    "\n",
    "The training was stable with no wild fluctuations or divergence:\n",
    "- Epoch 0:   Val R²: -0.41 (terrible)\n",
    "- Epoch 200: Val R²: -0.02 (almost zero)  \n",
    "- Epoch 1800: Val R²: -0.007 (basically zero)\n",
    "\n",
    "There was also no wild fluctuations or divergence\n",
    "- Training loss: 0.70 → 0.50 (steady decrease)\n",
    "\n",
    "There could be several reasons why this model is underperforming so drastically. There might not be any meaningful relationships, the learning rate could be too low, or there needs to be more feature engineering.\n",
    "\n",
    "I am going to try a higher learning rate for this exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "219ca37e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 | Train Loss: 0.634064 | Val Loss: 0.636516 | Val R²: -0.2523\n",
      "Epoch 200 | Train Loss: 0.497367 | Val Loss: 0.512433 | Val R²: -0.0082\n",
      "Epoch 400 | Train Loss: 0.496157 | Val Loss: 0.511954 | Val R²: -0.0072\n",
      "Epoch 600 | Train Loss: 0.495554 | Val Loss: 0.511764 | Val R²: -0.0069\n",
      "Epoch 800 | Train Loss: 0.495086 | Val Loss: 0.511590 | Val R²: -0.0065\n",
      "Epoch 1000 | Train Loss: 0.494719 | Val Loss: 0.511444 | Val R²: -0.0062\n",
      "Epoch 1200 | Train Loss: 0.494374 | Val Loss: 0.511288 | Val R²: -0.0059\n",
      "Epoch 1400 | Train Loss: 0.494027 | Val Loss: 0.511110 | Val R²: -0.0056\n",
      "Epoch 1600 | Train Loss: 0.493670 | Val Loss: 0.510894 | Val R²: -0.0052\n",
      "Epoch 1800 | Train Loss: 0.493202 | Val Loss: 0.510678 | Val R²: -0.0047\n",
      "\n",
      "Final Result\n",
      "Training MSE: 0.985601 | R2 0.0078\n",
      "Validation MSE: 1.021333 | R2: -0.0047\n",
      "Test MSE: 0.996995 | R2: 0.0054\n"
     ]
    }
   ],
   "source": [
    "# Initialize network \n",
    "nn = RegressionNeuralNetwork(input_size, hidden_size, output_size)\n",
    "\n",
    "# Train the network\n",
    "training_history = nn.train(x_train, y_train, x_val,y_val, epochs=2000, learning_rate=0.05, early_stopping_patience=300)\n",
    "\n",
    "# Evaluate performance on all three sets\n",
    "train_mse = nn.mse(x_train, y_train)\n",
    "val_mse = nn.mse(x_val,y_val)\n",
    "test_mse = nn.mse(x_test, y_test)\n",
    "\n",
    "train_r2 = nn.r2_score(x_train, y_train)\n",
    "val_r2 = nn.r2_score(x_val,y_val)\n",
    "test_r2 = nn.r2_score(x_test,y_test)\n",
    "\n",
    "print(f\"\\nFinal Result\")\n",
    "print(f\"Training MSE: {train_mse:.6f} | R2 {train_r2:.4f}\")\n",
    "print(f\"Validation MSE: {val_mse:.6f} | R2: {val_r2:.4f}\")\n",
    "print(f\"Test MSE: {test_mse:.6f} | R2: {test_r2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeede054",
   "metadata": {},
   "source": [
    "The results do show improvement:\n",
    "With a higher learning rate there was faster initial learning as shown below:\n",
    "- Previous (LR=0.01): Epoch 0: Val R² = -0.41\n",
    "- Current (LR=0.05):  Epoch 0: Val R² = -0.25  # Better starting point\n",
    "\n",
    "The new method also shows more learning progress:\n",
    "- Previous: Train loss: 0.70 → 0.50 (28% decrease)\n",
    "- Current:  Train loss: 0.63 → 0.49 (22% decrease, but faster)\n",
    "\n",
    "It is also showing continued improvement:\n",
    "- Epoch 1800: Val R² = -0.0047 (approaching zero) # Still learning slowly but steadily\n",
    "\n",
    "However, the core issue persists::\n",
    "- R² = -0.0047 ≈ 0\n",
    "\n",
    "The model predictions are essentially the same as just predicting the average (309 minutes) for every user.\n",
    "\n",
    "So from here it's safe to say we can try something else:\n",
    "- Possibly try different features\n",
    "- Try different targets\n",
    "- Get more data \n",
    "- Accept the results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d78ad20",
   "metadata": {},
   "source": [
    "## Make the prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "534319ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model shows good generalization\n",
      "\n",
      "Best epoch: 1999\n",
      "Training stopped after 2000 epochs\n",
      "Completed all epochs without early stopping\n",
      "\n",
      "============================================================\n",
      "PREDICTIONS BY AGE GROUP\n",
      "============================================================\n",
      "Using average engagement metrics:\n",
      "Discover Engagement: 42.4%\n",
      "Songs Liked: 50\n",
      "Repeat Rate: 253.5%\n",
      "\n",
      "Age   Predicted Minutes  Actual Range   \n",
      "---------------------------------------------\n",
      "18    312                10-597\n",
      "25    307                10-600\n",
      "35    304                10-599\n",
      "45    310                11-600\n",
      "55    316                10-600\n"
     ]
    }
   ],
   "source": [
    "# Check for overfitting\n",
    "if train_r2 - test_r2 > 0.1:\n",
    "    print(\"Model may be overfitting (train-test R² gap > 0.1)\")\n",
    "else:\n",
    "    print(\"Model shows good generalization\")\n",
    "\n",
    "# Safe access to training history with fallbacks\n",
    "best_epoch = training_history.get('best_epoch', len(training_history['train_losses']) - 1)\n",
    "total_epochs = len(training_history['train_losses'])\n",
    "\n",
    "print(f\"\\nBest epoch: {best_epoch}\")\n",
    "print(f\"Training stopped after {total_epochs} epochs\")\n",
    "\n",
    "# Add training summary\n",
    "if best_epoch < total_epochs - 1:\n",
    "    print(f\"Early stopping activated (stopped {total_epochs - best_epoch - 1} epochs early)\")\n",
    "else:\n",
    "    print(\"Completed all epochs without early stopping\")\n",
    "\n",
    "# Show predictions for different age groups\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PREDICTIONS BY AGE GROUP\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create test cases for different ages with average engagement\n",
    "age_groups = [18, 25, 35, 45, 55]\n",
    "avg_discover = x_raw[:, 1].mean()  # Average discover engagement\n",
    "avg_songs = x_raw[:, 2].mean()     # Average songs liked\n",
    "avg_repeat = x_raw[:, 3].mean()    # Average repeat rate\n",
    "\n",
    "print(f\"Using average engagement metrics:\")\n",
    "print(f\"Discover Engagement: {avg_discover:.1f}%\")\n",
    "print(f\"Songs Liked: {avg_songs:.0f}\")\n",
    "print(f\"Repeat Rate: {avg_repeat:.1f}%\")\n",
    "\n",
    "print(f\"\\n{'Age':<5} {'Predicted Minutes':<18} {'Actual Range':<15}\")\n",
    "print(\"-\" * 45)\n",
    "\n",
    "for age in age_groups:\n",
    "    # Create input vector: [age, avg_discover, avg_songs, avg_repeat]\n",
    "    age_input = np.array([[age, avg_discover, avg_songs, avg_repeat]])\n",
    "    \n",
    "    # Scale the input\n",
    "    age_input_scaled = scaler_features.transform(age_input)\n",
    "    \n",
    "    # Predict\n",
    "    prediction_scaled = nn.predict(age_input_scaled)\n",
    "    prediction_minutes = scaler_target.inverse_transform(prediction_scaled)[0, 0]\n",
    "    \n",
    "    # Find actual range for this age group (±2 years)\n",
    "    age_mask = (music_data['age'] >= age-2) & (music_data['age'] <= age+2)\n",
    "    actual_range = music_data[age_mask]['minutes_streamed_per_day']\n",
    "    \n",
    "    if len(actual_range) > 0:\n",
    "        actual_min, actual_max = actual_range.min(), actual_range.max()\n",
    "        print(f\"{age:<5} {prediction_minutes:<18.0f} {actual_min:.0f}-{actual_max:.0f}\")\n",
    "    else:\n",
    "        print(f\"{age:<5} {prediction_minutes:<18.0f} No data\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
