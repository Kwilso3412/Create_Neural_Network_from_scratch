{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2440b718",
   "metadata": {},
   "source": [
    "# The Problem: What genre do the people in their country prefer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b13981b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12bb08e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country</th>\n",
       "      <th>top_genre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Japan</td>\n",
       "      <td>Reggae</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  country top_genre\n",
       "0   Japan    Reggae"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data \n",
    "music_data = pd.read_csv('../data/clean_data/country_genre.csv')\n",
    "music_data.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c45a87d",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e83db0f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Australia', 'Brazil', 'Canada', 'France', 'Germany', 'India',\n",
       "       'Japan', 'South Korea', 'UK', 'USA'], dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find all the unique countries \n",
    "countries = music_data['country'].values\n",
    "np.unique(countries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "737fc345",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Classical', 'Country', 'EDM', 'Hip-Hop', 'Jazz', 'Metal', 'Pop',\n",
       "       'R&B', 'Reggae', 'Rock'], dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find all unique genres \n",
    "genres = music_data['top_genre'].values\n",
    "np.unique(genres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9c42a646",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: (5000, 10)\n",
      "Output shape: (5000, 10)\n"
     ]
    }
   ],
   "source": [
    "# one-hot encode function\n",
    "def one_hot_encode(data, num_classes):\n",
    "    encoded = np.zeros((len(data), num_classes))\n",
    "    encoded[np.arange(len(data)), data] = 1\n",
    "    return encoded\n",
    "\n",
    "# encode the categorical data \n",
    "country_encoder = LabelEncoder()\n",
    "genre_encoder = LabelEncoder()\n",
    "\n",
    "country_encoded = country_encoder.fit_transform(countries)\n",
    "genre_encoded = genre_encoder.fit_transform(genres)\n",
    "\n",
    "\n",
    "# Create one-hot encoded features \n",
    "x = one_hot_encode(country_encoded, len(country_encoder.classes_))  # input: Countries\n",
    "y = one_hot_encode(genre_encoded, len(genre_encoder.classes_))      # output: Genres\n",
    "\n",
    "print(f\"Input shape: {x.shape}\") # Should be (5000,10)\n",
    "print(f\"Output shape: {y.shape}\") # Should be (5000,10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6a8a20d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: 3000 samples\n",
      "Validation set: 1000 samples\n",
      "Test set: 1000 samples\n"
     ]
    }
   ],
   "source": [
    "# split the data (train 60% , validate 20%, test 20%)\n",
    "x_train, x_temp, y_train, y_temp = train_test_split(x,y, test_size=0.4, random_state=42)\n",
    "x_val,x_test,y_val,y_test = train_test_split(x_temp,y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "print(f'Training set: {x_train.shape[0]} samples')\n",
    "print(f'Validation set: {x_val.shape[0]} samples')\n",
    "print(f'Test set: {x_test.shape[0]} samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63198b9b",
   "metadata": {},
   "source": [
    "## Creation of Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0a70e9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creation of the Neural Network class \n",
    "class ClassificationNeuralNetwork:\n",
    "    \"\"\"\n",
    "    2-layer Neural Network specialized for Classification tasks\n",
    "    - Multi-class output with softmax activation\n",
    "    - Cross-entropy loss function\n",
    "    - Accuracy evaluation metric\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        \"\"\"\n",
    "        Initialize classification neural network\n",
    "        \n",
    "        Args:\n",
    "            input_size: Number of input features\n",
    "            hidden_size: Number of hidden layer neurons\n",
    "            output_size: Number of classes to predict\n",
    "        \"\"\"\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        # Xavier initialization for stable training\n",
    "        self.W1 = np.random.randn(input_size, hidden_size) / np.sqrt(input_size)\n",
    "        self.b1 = np.zeros((1, hidden_size))\n",
    "        \n",
    "        self.W2 = np.random.randn(hidden_size, output_size) / np.sqrt(hidden_size)\n",
    "        self.b2 = np.zeros((1, output_size))\n",
    "        \n",
    "    def relu(self, x):\n",
    "        \"\"\"ReLU activation: f(x) = max(0, x)\"\"\"\n",
    "        return np.maximum(0, x)\n",
    "    \n",
    "    def relu_derivative(self, x):\n",
    "        \"\"\"ReLU derivative: f'(x) = 1 if x > 0, else 0\"\"\"\n",
    "        return (x > 0).astype(float)\n",
    "    \n",
    "    def softmax(self, x):\n",
    "        \"\"\"\n",
    "        Softmax activation for multi-class classification\n",
    "        Converts logits to probability distribution\n",
    "        \"\"\"\n",
    "        exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "        return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "    \n",
    "    def forward_pass(self, X):\n",
    "        \"\"\"\n",
    "        Forward propagation for classification:\n",
    "        z1 = X @ W1 + b1\n",
    "        a1 = ReLU(z1)\n",
    "        z2 = a1 @ W2 + b2\n",
    "        a2 = softmax(z2)  # Probability distribution\n",
    "        \"\"\"\n",
    "        # Hidden layer\n",
    "        self.z1 = X @ self.W1 + self.b1\n",
    "        self.a1 = self.relu(self.z1)\n",
    "        \n",
    "        # Output layer with softmax\n",
    "        self.z2 = self.a1 @ self.W2 + self.b2\n",
    "        self.a2 = self.softmax(self.z2)\n",
    "        \n",
    "        return self.a2\n",
    "    \n",
    "    def cross_entropy_loss(self, y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Cross-entropy loss for multi-class classification:\n",
    "        L = -Σ(y_true * log(y_pred)) / m\n",
    "        \"\"\"\n",
    "        epsilon = 1e-15  # Prevent log(0)\n",
    "        y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "        \n",
    "        m = y_true.shape[0]\n",
    "        loss = -np.sum(y_true * np.log(y_pred)) / m\n",
    "        return loss\n",
    "    \n",
    "    def backward_pass(self, X, y_true):\n",
    "        \"\"\"\n",
    "        Backpropagation for classification:\n",
    "        \n",
    "        Output layer (softmax + cross-entropy):\n",
    "        dL/dz2 = a2 - y_true\n",
    "        \n",
    "        Hidden layer:\n",
    "        dL/da1 = dL/dz2 @ W2^T\n",
    "        dL/dz1 = dL/da1 * ReLU'(z1)\n",
    "        \"\"\"\n",
    "        m = X.shape[0]\n",
    "        \n",
    "        # Output layer gradients (softmax + cross-entropy derivative)\n",
    "        dz2 = self.a2 - y_true\n",
    "        dW2 = self.a1.T @ dz2 / m\n",
    "        db2 = np.sum(dz2, axis=0, keepdims=True) / m\n",
    "        \n",
    "        # Hidden layer gradients\n",
    "        da1 = dz2 @ self.W2.T\n",
    "        dz1 = da1 * self.relu_derivative(self.z1)\n",
    "        dW1 = X.T @ dz1 / m\n",
    "        db1 = np.sum(dz1, axis=0, keepdims=True) / m\n",
    "        \n",
    "        return dW1, db1, dW2, db2\n",
    "    \n",
    "    def train(self, X_train, y_train, X_val, y_val, epochs=1000, learning_rate=0.01, early_stopping_patience=50):\n",
    "        \"\"\"Train classification neural network with validation monitoring\"\"\"\n",
    "        train_losses = []\n",
    "        val_losses = []\n",
    "        val_accuracies = []\n",
    "        \n",
    "        best_val_loss = float('inf')\n",
    "        patience_counter = 0\n",
    "        best_weights = None\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            # Forward pass and training loss\n",
    "            y_pred_train = self.forward_pass(X_train)\n",
    "            train_loss = self.cross_entropy_loss(y_train, y_pred_train)\n",
    "            train_losses.append(train_loss)\n",
    "            \n",
    "            # Backward pass and weight updates\n",
    "            dW1, db1, dW2, db2 = self.backward_pass(X_train, y_train)\n",
    "            \n",
    "            self.W1 -= learning_rate * dW1\n",
    "            self.b1 -= learning_rate * db1\n",
    "            self.W2 -= learning_rate * dW2\n",
    "            self.b2 -= learning_rate * db2\n",
    "            \n",
    "            # Validation evaluation\n",
    "            y_pred_val = self.forward_pass(X_val)\n",
    "            val_loss = self.cross_entropy_loss(y_val, y_pred_val)\n",
    "            val_losses.append(val_loss)\n",
    "            \n",
    "            val_accuracy = self.accuracy(X_val, y_val)\n",
    "            val_accuracies.append(val_accuracy)\n",
    "            \n",
    "            # Early stopping logic\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                patience_counter = 0\n",
    "                best_weights = {\n",
    "                    'W1': self.W1.copy(), 'b1': self.b1.copy(),\n",
    "                    'W2': self.W2.copy(), 'b2': self.b2.copy()\n",
    "                }\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "            \n",
    "            if epoch % 100 == 0:\n",
    "                print(f\"Epoch {epoch} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | Val Acc: {val_accuracy:.4f}\")\n",
    "            \n",
    "            if patience_counter >= early_stopping_patience:\n",
    "                print(f\"Early stopping at epoch {epoch}. Best validation loss: {best_val_loss:.4f}\")\n",
    "                self.W1, self.b1 = best_weights['W1'], best_weights['b1']\n",
    "                self.W2, self.b2 = best_weights['W2'], best_weights['b2']\n",
    "                break\n",
    "        \n",
    "        return {\n",
    "            'train_losses': train_losses,\n",
    "            'val_losses': val_losses,\n",
    "            'val_accuracies': val_accuracies\n",
    "        }\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Get class predictions (argmax of probabilities)\"\"\"\n",
    "        probabilities = self.forward_pass(X)\n",
    "        return np.argmax(probabilities, axis=1)\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Get prediction probabilities\"\"\"\n",
    "        return self.forward_pass(X)\n",
    "    \n",
    "    def accuracy(self, X, y_true):\n",
    "        \"\"\"Calculate classification accuracy\"\"\"\n",
    "        predictions = self.predict(X)\n",
    "        y_true_classes = np.argmax(y_true, axis=1)\n",
    "        return np.mean(predictions == y_true_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b9bcd3",
   "metadata": {},
   "source": [
    "## Using the Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b37b811",
   "metadata": {},
   "source": [
    "Used Rule of Thumb: 2/3 Rule\n",
    "\n",
    "\n",
    "Country → Genre (16 neurons)\n",
    "\n",
    "- Input complexity: 10 countries with different music cultures\n",
    "- Output complexity: 10 genres with complex relationships\n",
    "- Data size: 5000 samples (enough to support 16 neurons)\n",
    "- Reasoning: Countries have nuanced cultural preferences that need more neurons to capture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4036f39b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Architecture: 10 -> 16 -> 10\n"
     ]
    }
   ],
   "source": [
    "# Network architecture \n",
    "input_size = x.shape[1]  # should be 10 (countries)\n",
    "hidden_size = 16         # Hidden layer neurons\n",
    "output_size = y.shape[1] # 10 should genres\n",
    "\n",
    "print(f\"Architecture: {input_size} -> {hidden_size} -> {output_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7e5d124a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 | Train Loss: 2.3196 | Val Loss: 2.3165 | Val Acc: 0.0890\n",
      "Epoch 100 | Train Loss: 2.3151 | Val Loss: 2.3140 | Val Acc: 0.0890\n",
      "Epoch 200 | Train Loss: 2.3119 | Val Loss: 2.3124 | Val Acc: 0.1100\n",
      "Epoch 300 | Train Loss: 2.3096 | Val Loss: 2.3114 | Val Acc: 0.1040\n",
      "Epoch 400 | Train Loss: 2.3078 | Val Loss: 2.3108 | Val Acc: 0.0980\n",
      "Epoch 500 | Train Loss: 2.3065 | Val Loss: 2.3104 | Val Acc: 0.0980\n",
      "Epoch 600 | Train Loss: 2.3054 | Val Loss: 2.3101 | Val Acc: 0.0980\n",
      "Epoch 700 | Train Loss: 2.3045 | Val Loss: 2.3099 | Val Acc: 0.1040\n",
      "Epoch 800 | Train Loss: 2.3037 | Val Loss: 2.3098 | Val Acc: 0.1040\n",
      "Epoch 900 | Train Loss: 2.3031 | Val Loss: 2.3097 | Val Acc: 0.1030\n",
      "\n",
      "FINAL RESULTS\n",
      "Training Accuracy: 0.1063\n",
      "Validation Accuracy: 0.1030\n",
      "Test Accuracy: 0.0880\n"
     ]
    }
   ],
   "source": [
    "# Initialize network\n",
    "nn = ClassificationNeuralNetwork(input_size, hidden_size, output_size)\n",
    "\n",
    "# Train the network with validation monitoring\n",
    "training_history = nn.train(x_train, y_train, x_val, y_val, epochs=1000, learning_rate=0.01, early_stopping_patience=50)\n",
    "\n",
    "# Evaluate performance on all three sets \n",
    "train_accuracy = nn.accuracy(x_train, y_train)\n",
    "val_accuracy = nn.accuracy(x_val, y_val)\n",
    "test_accuracy = nn.accuracy(x_test,y_test)\n",
    "\n",
    "print('\\nFINAL RESULTS')\n",
    "print(f'Training Accuracy: {train_accuracy:.4f}')\n",
    "print(f'Validation Accuracy: {val_accuracy:.4f}')\n",
    "print(f'Test Accuracy: {test_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abbf730e",
   "metadata": {},
   "source": [
    "## These results are negative results.\n",
    "With a random baseline of 10%, the training accuracy resulted in 10.6%, validation was 10.3%, and the test accuracy was only 8.8%. Essentially the network did not learn anything.\n",
    "The loss also saw a minimal decrease from 2.3196 → 2.3031 (only 0.02 improvement).\n",
    "\n",
    "## Some possible solutions:\n",
    "1. Going to a bigger network\n",
    "2. Higher Learning Rate\n",
    "3. More training\n",
    "4. Better patience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "665561e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Architecture: 10 -> 16 -> 10\n"
     ]
    }
   ],
   "source": [
    "# Network architecture \n",
    "input_size = x.shape[1]  # should be 10 (countries)\n",
    "hidden_size = 16        # Hidden layer neurons\n",
    "output_size = y.shape[1] # 10 should genres\n",
    "\n",
    "print(f\"Architecture: {input_size} -> {hidden_size} -> {output_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c80c7525",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 | Train Loss: 2.3287 | Val Loss: 2.3268 | Val Acc: 0.1080\n",
      "Epoch 100 | Train Loss: 2.3120 | Val Loss: 2.3170 | Val Acc: 0.1070\n",
      "Epoch 200 | Train Loss: 2.3060 | Val Loss: 2.3141 | Val Acc: 0.1070\n",
      "Epoch 300 | Train Loss: 2.3030 | Val Loss: 2.3126 | Val Acc: 0.1170\n",
      "Epoch 400 | Train Loss: 2.3010 | Val Loss: 2.3115 | Val Acc: 0.1160\n",
      "Epoch 500 | Train Loss: 2.2995 | Val Loss: 2.3106 | Val Acc: 0.1160\n",
      "Epoch 600 | Train Loss: 2.2984 | Val Loss: 2.3098 | Val Acc: 0.1190\n",
      "Epoch 700 | Train Loss: 2.2975 | Val Loss: 2.3093 | Val Acc: 0.1210\n",
      "Epoch 800 | Train Loss: 2.2967 | Val Loss: 2.3088 | Val Acc: 0.1210\n",
      "Epoch 900 | Train Loss: 2.2961 | Val Loss: 2.3085 | Val Acc: 0.1200\n",
      "Epoch 1000 | Train Loss: 2.2956 | Val Loss: 2.3082 | Val Acc: 0.1240\n",
      "Epoch 1100 | Train Loss: 2.2951 | Val Loss: 2.3080 | Val Acc: 0.1190\n",
      "Epoch 1200 | Train Loss: 2.2947 | Val Loss: 2.3079 | Val Acc: 0.1190\n",
      "Epoch 1300 | Train Loss: 2.2943 | Val Loss: 2.3078 | Val Acc: 0.1190\n",
      "Epoch 1400 | Train Loss: 2.2939 | Val Loss: 2.3077 | Val Acc: 0.1200\n",
      "Epoch 1500 | Train Loss: 2.2936 | Val Loss: 2.3076 | Val Acc: 0.1200\n",
      "Epoch 1600 | Train Loss: 2.2933 | Val Loss: 2.3076 | Val Acc: 0.1250\n",
      "Epoch 1700 | Train Loss: 2.2930 | Val Loss: 2.3077 | Val Acc: 0.1250\n",
      "Early stopping at epoch 1724. Best validation loss: 2.3076\n",
      "\n",
      "FINAL RESULTS\n",
      "Training Accuracy: 0.1240\n",
      "Validation Accuracy: 0.1200\n",
      "Test Accuracy: 0.1180\n"
     ]
    }
   ],
   "source": [
    "# Initialize network\n",
    "nn = ClassificationNeuralNetwork(input_size, hidden_size, output_size)\n",
    "\n",
    "# Train the network with validation monitoring\n",
    "training_history = nn.train(x_train, y_train, x_val, y_val, epochs=2000, learning_rate=0.05, early_stopping_patience=200)\n",
    "\n",
    "# Evaluate performance on all three sets \n",
    "train_accuracy = nn.accuracy(x_train, y_train)\n",
    "val_accuracy = nn.accuracy(x_val, y_val)\n",
    "test_accuracy = nn.accuracy(x_test,y_test)\n",
    "\n",
    "print('\\nFINAL RESULTS')\n",
    "print(f'Training Accuracy: {train_accuracy:.4f}')\n",
    "print(f'Validation Accuracy: {val_accuracy:.4f}')\n",
    "print(f'Test Accuracy: {test_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b199fe",
   "metadata": {},
   "source": [
    "## What I choose to do \n",
    "1. Kept the neurons the same. I noticed when I increased the neurons to 32 it resulted in more overfitting and less training\n",
    "1. Raised Learning Rate: 0.1 -> 0.5\n",
    "2. Raised training: 1000 -> 2000 epochs\n",
    "3. Better patience: 50 -> 200 epochs\n",
    "\n",
    "## Better but still not great results \n",
    "\n",
    "The training accuracy moved up to 12.4%, the validation accuracy moved up to 12%, but the test accuracy is 11.8%. These tiny gaps mean that there is healthy learning without memorization.\n",
    "Yet these results are still not nearly good enough to consider the model ready for production to make any good predictions.\n",
    "\n",
    "## Why is this occurring\n",
    "\n",
    "I decided to take another look at the data and found that the countries almost have no preference when it comes to music taste.\n",
    "The best country saw that Germany loves EDM, with 15.1 percent of people stating this was their top choice. But otherwise the average preference across each country was around 12.3%. This gives a data variance of 2.03 which is extremely low.\n",
    "Essentially countries like all genres almost equally, and there are barely any signals to learn from.\n",
    "\n",
    "## Was this a failure or a success?\n",
    "This was actually a success. The Neural Network was successfully built and is working properly.\n",
    "\n",
    "- There are no strong patterns in the data.\n",
    "- When the model was predicting randomly it gave baseline accuracy, signifying that with improvement we might see better results.\n",
    "- When the network received tiny improvements it reported 12% accuracy which represents weak signals in the data.\n",
    "\n",
    "So overall the issue is not the network but the data I am training on.\n",
    "\n",
    "## What are the possible solutions?\n",
    "1. Ask a different question\n",
    "2. Use multiple features to reveal any hidden patterns\n",
    "3. Accept the results\n",
    "4. Get more data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f7d31ee1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model shows good generalization\n",
      "\n",
      "Best epoch: N/A\n",
      "Training stopped after 1725 epochs\n",
      "\n",
      "==================================================\n",
      "PREDICTIONS BY COUNTRY\n",
      "==================================================\n",
      "Australia: Reggae (11.99% confidence)\n",
      "Brazil: Rock (12.88% confidence)\n",
      "Canada: EDM (11.99% confidence)\n",
      "France: Jazz (12.66% confidence)\n",
      "Germany: EDM (11.99% confidence)\n",
      "India: Reggae (12.21% confidence)\n",
      "Japan: Jazz (10.75% confidence)\n",
      "South Korea: Rock (11.29% confidence)\n",
      "UK: Reggae (11.88% confidence)\n",
      "USA: Reggae (12.26% confidence)\n"
     ]
    }
   ],
   "source": [
    "# Check for overfitting\n",
    "if train_accuracy - test_accuracy > 0.1:\n",
    "    print(\"Model may be overfitting (train-test gap > 10%)\")\n",
    "else:\n",
    "    print(\"Model shows good generalization\")\n",
    "\n",
    "print(f'\\nBest epoch: {training_history[\"best_epoch\"] if \"best_epoch\" in training_history else \"N/A\"}')\n",
    "print(f'Training stopped after {len(training_history[\"train_losses\"])} epochs')\n",
    "\n",
    "# Show genre preferences by country (for classification)\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"PREDICTIONS BY COUNTRY\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Create input for each country \n",
    "country_inputs = np.eye(len(country_encoder.classes_))\n",
    "genre_probs = nn.predict_proba(country_inputs)  # Use predict_proba for cleaner interface\n",
    "\n",
    "for i, country in enumerate(country_encoder.classes_):\n",
    "    predicted_genre_idx = np.argmax(genre_probs[i])\n",
    "    predicted_genre = genre_encoder.classes_[predicted_genre_idx]\n",
    "    confidence = genre_probs[i][predicted_genre_idx]\n",
    "\n",
    "    print(f\"{country}: {predicted_genre} ({confidence:.2%} confidence)\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
