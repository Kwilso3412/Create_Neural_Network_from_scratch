{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2440b718",
   "metadata": {},
   "source": [
    "# The Problem: What genre do the people in their country prefer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b13981b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12bb08e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country</th>\n",
       "      <th>top_genre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Japan</td>\n",
       "      <td>Reggae</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  country top_genre\n",
       "0   Japan    Reggae"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data \n",
    "music_data = pd.read_csv('../data/clean_data/country_genre.csv')\n",
    "music_data.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c45a87d",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e83db0f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Australia', 'Brazil', 'Canada', 'France', 'Germany', 'India',\n",
       "       'Japan', 'South Korea', 'UK', 'USA'], dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find all the unique countries \n",
    "countries = music_data['country'].values\n",
    "np.unique(countries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "737fc345",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Classical', 'Country', 'EDM', 'Hip-Hop', 'Jazz', 'Metal', 'Pop',\n",
       "       'R&B', 'Reggae', 'Rock'], dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find all unique genres \n",
    "genres = music_data['top_genre'].values\n",
    "np.unique(genres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9c42a646",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: (5000, 10)\n",
      "Output shape: (5000, 10)\n"
     ]
    }
   ],
   "source": [
    "# one-hot encode function\n",
    "def one_hot_encode(data, num_classes):\n",
    "    encoded = np.zeros((len(data), num_classes))\n",
    "    encoded[np.arange(len(data)), data] = 1\n",
    "    return encoded\n",
    "\n",
    "# encode the categorical data \n",
    "country_encoder = LabelEncoder()\n",
    "genre_encoder = LabelEncoder()\n",
    "\n",
    "country_encoded = country_encoder.fit_transform(countries)\n",
    "genre_encoded = genre_encoder.fit_transform(genres)\n",
    "\n",
    "\n",
    "# Create one-hot encoded features \n",
    "x = one_hot_encode(country_encoded, len(country_encoder.classes_))  # input: Countries\n",
    "y = one_hot_encode(genre_encoded, len(genre_encoder.classes_))      # output: Genres\n",
    "\n",
    "print(f\"Input shape: {x.shape}\") # Should be (5000,10)\n",
    "print(f\"Output shape: {y.shape}\") # Should be (5000,10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c344c5e",
   "metadata": {},
   "source": [
    "### Creating training data (train, validate, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6a8a20d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: 3000 samples\n",
      "Validation set: 1000 samples\n",
      "Test set: 1000 samples\n"
     ]
    }
   ],
   "source": [
    "# split the data (train 60% , validate 20%, test 20%)\n",
    "x_train, x_temp, y_train, y_temp = train_test_split(x,y, test_size=0.4, random_state=42)\n",
    "x_val,x_test,y_val,y_test = train_test_split(x_temp,y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "print(f'Training set: {x_train.shape[0]} samples')\n",
    "print(f'Validation set: {x_val.shape[0]} samples')\n",
    "print(f'Test set: {x_test.shape[0]} samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63198b9b",
   "metadata": {},
   "source": [
    "## Creation of Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0a70e9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creation of the Neural Network class \n",
    "class NeuralNetwork:\n",
    "    \"\"\"\n",
    "    2-layer Neural Network from scratch\n",
    "    Architecture: Input -> Hidden -> Output\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        # Initialize weights with small random values\n",
    "        # xavier initialization: weights ~ N(0, 1/sqrt(fan_in))\n",
    "        self.W1 = np.random.randn(input_size, hidden_size) / np.sqrt(input_size)\n",
    "        self.b1 = np.zeros((1, hidden_size))\n",
    "        \n",
    "        self.W2 = np.random.randn(hidden_size, output_size) / np.sqrt(hidden_size)\n",
    "        self.b2 = np.zeros((1, output_size))\n",
    "        \n",
    "    def relu(self, x):\n",
    "        \"\"\"ReLU activation: f(x) = max(0, x)\"\"\"\n",
    "        return np.maximum(0, x)\n",
    "    \n",
    "    def relu_derivative(self, x):\n",
    "        \"\"\"ReLU derivative: f'(x) = 1 if x > 0, else 0\"\"\"\n",
    "        return (x > 0).astype(float)\n",
    "    \n",
    "    def softmax(self, x):\n",
    "        \"\"\"Softmax activation for multi-class classification\"\"\"\n",
    "        # Subtract max for numerical stability\n",
    "        exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "        return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "    \n",
    "    def forward_pass(self, x):\n",
    "        \"\"\"\n",
    "        Forward propagation:\n",
    "        z1 = x @ W1 + b1\n",
    "        a1 = ReLU(z1)\n",
    "        z2 = a1 @ W2 + b2\n",
    "        a2 = softmax(z2)\n",
    "        \"\"\"\n",
    "        # Hidden layer\n",
    "        self.z1 = x @ self.W1 + self.b1\n",
    "        self.a1 = self.relu(self.z1)\n",
    "        \n",
    "        # Output layer\n",
    "        self.z2 = self.a1 @ self.W2 + self.b2\n",
    "        self.a2 = self.softmax(self.z2)\n",
    "        \n",
    "        return self.a2\n",
    "    \n",
    "    def compute_loss(self, y_true, y_pred):\n",
    "        \"\"\"Cross-entropy loss: L = -Σ(y_true * log(y_pred))\"\"\"\n",
    "        # Add small epsilon to prevent log(0)\n",
    "        epsilon = 1e-15\n",
    "        y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "        \n",
    "        m = y_true.shape[0]\n",
    "        loss = -np.sum(y_true * np.log(y_pred)) / m\n",
    "        return loss\n",
    "    \n",
    "    def backward_pass(self, x, y_true):\n",
    "        \"\"\"\n",
    "        Backpropagation using chain rule:\n",
    "        \n",
    "        1. Output layer gradients:\n",
    "            dL/dz2 = a2 - y_true (softmax + cross-entropy derivative)\n",
    "            dL/dW2 = a1^T @ (a2 - y_true)\n",
    "            dL/db2 = sum(a2 - y_true)\n",
    "        \n",
    "        2. Hidden layer gradients:\n",
    "            dL/da1 = (a2 - y_true) @ W2^T\n",
    "            dL/dz1 = dL/da1 * ReLU'(z1)\n",
    "            dL/dW1 = x^T @ dL/dz1\n",
    "            dL/db1 = sum(dL/dz1)\n",
    "        \"\"\"\n",
    "        m = x.shape[0]\n",
    "        \n",
    "        # Output layer gradients\n",
    "        dz2 = self.a2 - y_true\n",
    "        dW2 = self.a1.T @ dz2 / m\n",
    "        db2 = np.sum(dz2, axis=0, keepdims=True) / m\n",
    "        \n",
    "        # Hidden layer gradients\n",
    "        da1 = dz2 @ self.W2.T\n",
    "        dz1 = da1 * self.relu_derivative(self.z1)\n",
    "        dW1 = x.T @ dz1 / m\n",
    "        db1 = np.sum(dz1, axis=0, keepdims=True) / m\n",
    "        \n",
    "        return dW1, db1, dW2, db2\n",
    "    \n",
    "    def train(self, x_train, y_train, x_val, y_val, epochs=1000, learning_rate=0.01, early_stopping_patience=50):\n",
    "        \"\"\"Train the neural network with validation monitoring and early stopping\"\"\"\n",
    "        train_losses = []\n",
    "        val_losses = []\n",
    "        val_accuracies = []\n",
    "        \n",
    "        best_val_loss = float('inf')\n",
    "        patience_counter = 0\n",
    "        best_weights = None\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            # Forward pass on training data\n",
    "            y_pred_train = self.forward_pass(x_train)\n",
    "            \n",
    "            # Compute training loss\n",
    "            train_loss = self.compute_loss(y_train, y_pred_train)\n",
    "            train_losses.append(train_loss)\n",
    "            \n",
    "            # Backward pass and weight updates\n",
    "            dW1, db1, dW2, db2 = self.backward_pass(x_train, y_train)\n",
    "            \n",
    "            # Update weights using gradient descent:\n",
    "            # W = W - learning_rate * dL/dW\n",
    "            self.W1 -= learning_rate * dW1\n",
    "            self.b1 -= learning_rate * db1\n",
    "            self.W2 -= learning_rate * dW2\n",
    "            self.b2 -= learning_rate * db2\n",
    "            \n",
    "            # Validation evaluation\n",
    "            y_pred_val = self.forward_pass(x_val)\n",
    "            val_loss = self.compute_loss(y_val, y_pred_val)\n",
    "            val_losses.append(val_loss)\n",
    "            \n",
    "            val_accuracy = self.accuracy(x_val, y_val)\n",
    "            val_accuracies.append(val_accuracy)\n",
    "            \n",
    "            # Early stopping logic\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                patience_counter = 0\n",
    "                # Save best weights\n",
    "                best_weights = {\n",
    "                    'W1': self.W1.copy(),\n",
    "                    'b1': self.b1.copy(),\n",
    "                    'W2': self.W2.copy(),\n",
    "                    'b2': self.b2.copy()\n",
    "                }\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "            \n",
    "            if epoch % 100 == 0:\n",
    "                print(f\"Epoch {epoch} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | Val Acc: {val_accuracy:.4f}\")\n",
    "            \n",
    "            # Early stopping\n",
    "            if patience_counter >= early_stopping_patience:\n",
    "                print(f\"Early stopping at epoch {epoch}. Best validation loss: {best_val_loss:.4f}\")\n",
    "                # Restore best weights\n",
    "                self.W1 = best_weights['W1']\n",
    "                self.b1 = best_weights['b1']\n",
    "                self.W2 = best_weights['W2']\n",
    "                self.b2 = best_weights['b2']\n",
    "                break\n",
    "        \n",
    "        return {\n",
    "            'train_losses': train_losses,\n",
    "            'val_losses': val_losses,\n",
    "            'val_accuracies': val_accuracies,\n",
    "            'best_epoch': len(train_losses) - patience_counter - 1\n",
    "        }\n",
    "    \n",
    "    def predict(self, x):\n",
    "        \"\"\"Make predictions on new data\"\"\"\n",
    "        predictions = self.forward_pass(x)\n",
    "        return np.argmax(predictions, axis=1)\n",
    "    \n",
    "    def accuracy(self, x, y):\n",
    "        \"\"\"Calculate accuracy\"\"\"\n",
    "        predictions = self.predict(x)\n",
    "        y_true = np.argmax(y, axis=1)\n",
    "        return np.mean(predictions == y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b9bcd3",
   "metadata": {},
   "source": [
    "## Using the Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b37b811",
   "metadata": {},
   "source": [
    "Used Rule of Thumb: 2/3 Rule\n",
    "\n",
    "\n",
    "Country → Genre (16 neurons)\n",
    "\n",
    "- Input complexity: 10 countries with different music cultures\n",
    "- Output complexity: 10 genres with complex relationships\n",
    "- Data size: 5000 samples (enough to support 16 neurons)\n",
    "- Reasoning: Countries have nuanced cultural preferences that need more neurons to capture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4036f39b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Architecture: 10 -> 16 -> 10\n"
     ]
    }
   ],
   "source": [
    "# Network architecture \n",
    "input_size = x.shape[1]  # should be 10 (countries)\n",
    "hidden_size = 16         # Hidden layer neurons\n",
    "output_size = y.shape[1] # 10 should genres\n",
    "\n",
    "print(f\"Architecture: {input_size} -> {hidden_size} -> {output_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7e5d124a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 | Train Loss: 2.3280 | Val Loss: 2.3243 | Val Acc: 0.1160\n",
      "Epoch 100 | Train Loss: 2.3210 | Val Loss: 2.3200 | Val Acc: 0.1120\n",
      "Epoch 200 | Train Loss: 2.3165 | Val Loss: 2.3176 | Val Acc: 0.1100\n",
      "Epoch 300 | Train Loss: 2.3135 | Val Loss: 2.3162 | Val Acc: 0.1070\n",
      "Epoch 400 | Train Loss: 2.3114 | Val Loss: 2.3153 | Val Acc: 0.1070\n",
      "Epoch 500 | Train Loss: 2.3098 | Val Loss: 2.3147 | Val Acc: 0.1070\n",
      "Epoch 600 | Train Loss: 2.3086 | Val Loss: 2.3143 | Val Acc: 0.1070\n",
      "Epoch 700 | Train Loss: 2.3076 | Val Loss: 2.3140 | Val Acc: 0.1090\n",
      "Epoch 800 | Train Loss: 2.3068 | Val Loss: 2.3138 | Val Acc: 0.1090\n",
      "Epoch 900 | Train Loss: 2.3061 | Val Loss: 2.3136 | Val Acc: 0.1020\n",
      "\n",
      "FINAL RESULTS\n",
      "Training Accuracy: 0.1010\n",
      "Validation Accuracy: 0.1030\n",
      "Test Accuracy: 0.0970\n"
     ]
    }
   ],
   "source": [
    "# Initialize network\n",
    "nn = NeuralNetwork(input_size, hidden_size, output_size)\n",
    "\n",
    "# Train the network with validation monitoring\n",
    "training_history = nn.train(x_train, y_train, x_val, y_val, epochs=1000, learning_rate=0.01, early_stopping_patience=50)\n",
    "\n",
    "# Evaluate performance on all three sets \n",
    "train_accuracy = nn.accuracy(x_train, y_train)\n",
    "val_accuracy = nn.accuracy(x_val, y_val)\n",
    "test_accuracy = nn.accuracy(x_test,y_test)\n",
    "\n",
    "print('\\nFINAL RESULTS')\n",
    "print(f'Training Accuracy: {train_accuracy:.4f}')\n",
    "print(f'Validation Accuracy: {val_accuracy:.4f}')\n",
    "print(f'Test Accuracy: {test_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abbf730e",
   "metadata": {},
   "source": [
    "## These results are negative results.\n",
    "\n",
    "I expected the performance to be 10% accurate  (1 out of 10). But in reality I got -10% accuracy, so basically the model learned nothing.\n",
    "\n",
    "## Some big red flags\n",
    "- The loss barely decreases from 2.328 -> 2.306\n",
    "- random-level accuracy: All -10% (train/val/test)\n",
    "- Flat learning curve: No real improvement after 900 epochs\n",
    "\n",
    "## Some possible solutions:\n",
    "1. Going to a bigger network:  16 -> 32 neurons \n",
    "2. Higher Learning Rate: 0.1 - 0.5\n",
    "3. More training: 1000 -> 2000 epochs\n",
    "4. Better patience: 50 -> 200 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c80c7525",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 | Train Loss: 2.3191 | Val Loss: 2.3342 | Val Acc: 0.0990\n",
      "Epoch 100 | Train Loss: 2.3089 | Val Loss: 2.3184 | Val Acc: 0.1060\n",
      "Epoch 200 | Train Loss: 2.3053 | Val Loss: 2.3136 | Val Acc: 0.1060\n",
      "Epoch 300 | Train Loss: 2.3031 | Val Loss: 2.3115 | Val Acc: 0.0940\n",
      "Epoch 400 | Train Loss: 2.3014 | Val Loss: 2.3105 | Val Acc: 0.0950\n",
      "Epoch 500 | Train Loss: 2.3002 | Val Loss: 2.3098 | Val Acc: 0.0950\n",
      "Epoch 600 | Train Loss: 2.2992 | Val Loss: 2.3094 | Val Acc: 0.0950\n",
      "Epoch 700 | Train Loss: 2.2984 | Val Loss: 2.3091 | Val Acc: 0.0950\n",
      "Epoch 800 | Train Loss: 2.2977 | Val Loss: 2.3089 | Val Acc: 0.0950\n",
      "Epoch 900 | Train Loss: 2.2972 | Val Loss: 2.3088 | Val Acc: 0.0940\n",
      "Epoch 1000 | Train Loss: 2.2967 | Val Loss: 2.3087 | Val Acc: 0.0980\n",
      "Epoch 1100 | Train Loss: 2.2962 | Val Loss: 2.3087 | Val Acc: 0.0990\n",
      "Epoch 1200 | Train Loss: 2.2958 | Val Loss: 2.3086 | Val Acc: 0.0930\n",
      "Epoch 1300 | Train Loss: 2.2955 | Val Loss: 2.3086 | Val Acc: 0.0930\n",
      "Epoch 1400 | Train Loss: 2.2951 | Val Loss: 2.3085 | Val Acc: 0.0930\n",
      "Epoch 1500 | Train Loss: 2.2948 | Val Loss: 2.3085 | Val Acc: 0.0930\n",
      "Epoch 1600 | Train Loss: 2.2945 | Val Loss: 2.3086 | Val Acc: 0.0940\n",
      "Early stopping at epoch 1660. Best validation loss: 2.3085\n",
      "\n",
      "FINAL RESULTS\n",
      "Training Accuracy: 0.1207\n",
      "Validation Accuracy: 0.0930\n",
      "Test Accuracy: 0.1040\n"
     ]
    }
   ],
   "source": [
    "# Initialize network\n",
    "nn = NeuralNetwork(input_size, hidden_size, output_size)\n",
    "\n",
    "# Train the network with validation monitoring\n",
    "training_history = nn.train(x_train, y_train, x_val, y_val, epochs=2000, learning_rate=0.05, early_stopping_patience=200)\n",
    "\n",
    "# Evaluate performance on all three sets \n",
    "train_accuracy = nn.accuracy(x_train, y_train)\n",
    "val_accuracy = nn.accuracy(x_val, y_val)\n",
    "test_accuracy = nn.accuracy(x_test,y_test)\n",
    "\n",
    "print('\\nFINAL RESULTS')\n",
    "print(f'Training Accuracy: {train_accuracy:.4f}')\n",
    "print(f'Validation Accuracy: {val_accuracy:.4f}')\n",
    "print(f'Test Accuracy: {test_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b199fe",
   "metadata": {},
   "source": [
    "## Better but still not great results \n",
    "\n",
    "The accuracy moved up to 12.1%, random baseline increased to 10.4% and the loss reduced to 2.29 which is still minimal learning. \n",
    "\n",
    "Yet these results are still not nearly as good enough to consider the model to be sent into production to make any good predictions. \n",
    "\n",
    "## Why is this occurring\n",
    "\n",
    "I decided to take another look a the data and found that the countries almost have no preference when it comes to music taste. \n",
    "\n",
    "The best country saw that Germany loves EDM, with 15.1 percent of people stating this was their top choice. But otherwise the average preference across each country was around 12.3%. This gives a data variance of 2.03 which is extremely low. \n",
    "\n",
    "Essentially countries like all genres almost equally, and there are barely any signals to learn from. \n",
    "\n",
    "## Was this a failure or a success?\n",
    "This was a actually success. The Neural Network was successfully built and is working properly.\n",
    "\n",
    "- There are no strong patterns in teh data \n",
    "- When the model was prediction randomly it gave -10 accuracy signifying that with improvement we might see more better results.\n",
    "- When teh network received tiny improvements it reported 12% accuracy which represents weak signals in the data. \n",
    "\n",
    "So overall the issue is not the network but the data I am training on. \n",
    "\n",
    "## What are the possible solutions?\n",
    "1. Ask a different question\n",
    "2. Use multiple features to reveal any hidden patterns\n",
    "3. Accept the results\n",
    "4. Get more data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f7d31ee1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model shows good generalization\n",
      "\n",
      "Best epoch: 1460\n",
      "Training stopped after 1661 epochs\n",
      "Australia: Classical (0.12 confidence)\n",
      "Brazil: R&B (0.11 confidence)\n",
      "Canada: R&B (0.11 confidence)\n",
      "France: Reggae (0.12 confidence)\n",
      "Germany: Metal (0.11 confidence)\n",
      "India: Reggae (0.12 confidence)\n",
      "Japan: Jazz (0.11 confidence)\n",
      "South Korea: EDM (0.12 confidence)\n",
      "UK: Jazz (0.12 confidence)\n",
      "USA: Reggae (0.13 confidence)\n"
     ]
    }
   ],
   "source": [
    "# Check for overfitting\n",
    "if train_accuracy - test_accuracy > 0.1:\n",
    "    print(\"model may be overfitting (train-test gap> 10%)\")\n",
    "else:\n",
    "    print(\"Model shows good generalization\")\n",
    "\n",
    "print(f'\\nBest epoch: {training_history['best_epoch']}')\n",
    "print(f'Training stopped after {len(training_history['train_losses'])} epochs')\n",
    "\n",
    "# show genre preferences by country\n",
    "# create input for each country \n",
    "country_inputs = np.eye(len(country_encoder.classes_))\n",
    "genre_probs = nn.forward_pass(country_inputs)\n",
    "\n",
    "for i, country in enumerate(country_encoder.classes_):\n",
    "    predicted_genre_idx = np.argmax(genre_probs[i])\n",
    "    predicted_genre = genre_encoder.classes_[predicted_genre_idx]\n",
    "    confidence = genre_probs[i][predicted_genre_idx]\n",
    "\n",
    "    print(f\"{country}: {predicted_genre} ({confidence:.2f} confidence)\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
