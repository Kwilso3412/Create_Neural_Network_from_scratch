{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf7dd43a",
   "metadata": {},
   "source": [
    "# The Problem: What are by age what are the preferred listening times of users (Morning/Afternoon/Night)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d69914",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48396702",
   "metadata": {},
   "source": [
    "## Load Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581857b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea51b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creation of the Neural Network class \n",
    "class ClassificationNeuralNetwork:\n",
    "    \"\"\"\n",
    "    2-layer Neural Network specialized for Classification tasks\n",
    "    - Multi-class output with softmax activation\n",
    "    - Cross-entropy loss function\n",
    "    - Accuracy evaluation metric\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        \"\"\"\n",
    "        Initialize classification neural network\n",
    "        \n",
    "        Args:\n",
    "            input_size: Number of input features\n",
    "            hidden_size: Number of hidden layer neurons\n",
    "            output_size: Number of classes to predict\n",
    "        \"\"\"\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        # Xavier initialization for stable training\n",
    "        self.W1 = np.random.randn(input_size, hidden_size) / np.sqrt(input_size)\n",
    "        self.b1 = np.zeros((1, hidden_size))\n",
    "        \n",
    "        self.W2 = np.random.randn(hidden_size, output_size) / np.sqrt(hidden_size)\n",
    "        self.b2 = np.zeros((1, output_size))\n",
    "        \n",
    "    def relu(self, x):\n",
    "        \"\"\"ReLU activation: f(x) = max(0, x)\"\"\"\n",
    "        return np.maximum(0, x)\n",
    "    \n",
    "    def relu_derivative(self, x):\n",
    "        \"\"\"ReLU derivative: f'(x) = 1 if x > 0, else 0\"\"\"\n",
    "        return (x > 0).astype(float)\n",
    "    \n",
    "    def softmax(self, x):\n",
    "        \"\"\"\n",
    "        Softmax activation for multi-class classification\n",
    "        Converts logits to probability distribution\n",
    "        \"\"\"\n",
    "        exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "        return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "    \n",
    "    def forward_pass(self, X):\n",
    "        \"\"\"\n",
    "        Forward propagation for classification:\n",
    "        z1 = X @ W1 + b1\n",
    "        a1 = ReLU(z1)\n",
    "        z2 = a1 @ W2 + b2\n",
    "        a2 = softmax(z2)  # Probability distribution\n",
    "        \"\"\"\n",
    "        # Hidden layer\n",
    "        self.z1 = X @ self.W1 + self.b1\n",
    "        self.a1 = self.relu(self.z1)\n",
    "        \n",
    "        # Output layer with softmax\n",
    "        self.z2 = self.a1 @ self.W2 + self.b2\n",
    "        self.a2 = self.softmax(self.z2)\n",
    "        \n",
    "        return self.a2\n",
    "    \n",
    "    def cross_entropy_loss(self, y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Cross-entropy loss for multi-class classification:\n",
    "        L = -Σ(y_true * log(y_pred)) / m\n",
    "        \"\"\"\n",
    "        epsilon = 1e-15  # Prevent log(0)\n",
    "        y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "        \n",
    "        m = y_true.shape[0]\n",
    "        loss = -np.sum(y_true * np.log(y_pred)) / m\n",
    "        return loss\n",
    "    \n",
    "    def backward_pass(self, X, y_true):\n",
    "        \"\"\"\n",
    "        Backpropagation for classification:\n",
    "        \n",
    "        Output layer (softmax + cross-entropy):\n",
    "        dL/dz2 = a2 - y_true\n",
    "        \n",
    "        Hidden layer:\n",
    "        dL/da1 = dL/dz2 @ W2^T\n",
    "        dL/dz1 = dL/da1 * ReLU'(z1)\n",
    "        \"\"\"\n",
    "        m = X.shape[0]\n",
    "        \n",
    "        # Output layer gradients (softmax + cross-entropy derivative)\n",
    "        dz2 = self.a2 - y_true\n",
    "        dW2 = self.a1.T @ dz2 / m\n",
    "        db2 = np.sum(dz2, axis=0, keepdims=True) / m\n",
    "        \n",
    "        # Hidden layer gradients\n",
    "        da1 = dz2 @ self.W2.T\n",
    "        dz1 = da1 * self.relu_derivative(self.z1)\n",
    "        dW1 = X.T @ dz1 / m\n",
    "        db1 = np.sum(dz1, axis=0, keepdims=True) / m\n",
    "        \n",
    "        return dW1, db1, dW2, db2\n",
    "    \n",
    "    def train(self, X_train, y_train, X_val, y_val, epochs=1000, learning_rate=0.01, early_stopping_patience=50):\n",
    "        \"\"\"Train classification neural network with validation monitoring\"\"\"\n",
    "        train_losses = []\n",
    "        val_losses = []\n",
    "        val_accuracies = []\n",
    "        \n",
    "        best_val_loss = float('inf')\n",
    "        patience_counter = 0\n",
    "        best_weights = None\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            # Forward pass and training loss\n",
    "            y_pred_train = self.forward_pass(X_train)\n",
    "            train_loss = self.cross_entropy_loss(y_train, y_pred_train)\n",
    "            train_losses.append(train_loss)\n",
    "            \n",
    "            # Backward pass and weight updates\n",
    "            dW1, db1, dW2, db2 = self.backward_pass(X_train, y_train)\n",
    "            \n",
    "            self.W1 -= learning_rate * dW1\n",
    "            self.b1 -= learning_rate * db1\n",
    "            self.W2 -= learning_rate * dW2\n",
    "            self.b2 -= learning_rate * db2\n",
    "            \n",
    "            # Validation evaluation\n",
    "            y_pred_val = self.forward_pass(X_val)\n",
    "            val_loss = self.cross_entropy_loss(y_val, y_pred_val)\n",
    "            val_losses.append(val_loss)\n",
    "            \n",
    "            val_accuracy = self.accuracy(X_val, y_val)\n",
    "            val_accuracies.append(val_accuracy)\n",
    "            \n",
    "            # Early stopping logic\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                patience_counter = 0\n",
    "                best_weights = {\n",
    "                    'W1': self.W1.copy(), 'b1': self.b1.copy(),\n",
    "                    'W2': self.W2.copy(), 'b2': self.b2.copy()\n",
    "                }\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "            \n",
    "            if epoch % 100 == 0:\n",
    "                print(f\"Epoch {epoch} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | Val Acc: {val_accuracy:.4f}\")\n",
    "            \n",
    "            if patience_counter >= early_stopping_patience:\n",
    "                print(f\"Early stopping at epoch {epoch}. Best validation loss: {best_val_loss:.4f}\")\n",
    "                self.W1, self.b1 = best_weights['W1'], best_weights['b1']\n",
    "                self.W2, self.b2 = best_weights['W2'], best_weights['b2']\n",
    "                break\n",
    "        \n",
    "        return {\n",
    "            'train_losses': train_losses,\n",
    "            'val_losses': val_losses,\n",
    "            'val_accuracies': val_accuracies\n",
    "        }\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Get class predictions (argmax of probabilities)\"\"\"\n",
    "        probabilities = self.forward_pass(X)\n",
    "        return np.argmax(probabilities, axis=1)\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Get prediction probabilities\"\"\"\n",
    "        return self.forward_pass(X)\n",
    "    \n",
    "    def accuracy(self, X, y_true):\n",
    "        \"\"\"Calculate classification accuracy\"\"\"\n",
    "        predictions = self.predict(X)\n",
    "        y_true_classes = np.argmax(y_true, axis=1)\n",
    "        return np.mean(predictions == y_true_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ecbbba",
   "metadata": {},
   "source": [
    "Rule of Thumb: 2/3 Rule\n",
    "\n",
    "Country → Minutes (8 neurons)\n",
    "\n",
    "- Simpler problem: Predicting single number (total minutes)\n",
    "- Less complexity: Regression typically needs fewer neurons than classification\n",
    "- Reasoning: Total streaming time has fewer patterns than genre preferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa73c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for overfitting\n",
    "if train_accuracy - test_accuracy > 0.1:\n",
    "    print(\"Model may be overfitting (train-test gap > 10%)\")\n",
    "else:\n",
    "    print(\"Model shows good generalization\")\n",
    "\n",
    "print(f'\\nBest epoch: {training_history[\"best_epoch\"] if \"best_epoch\" in training_history else \"N/A\"}')\n",
    "print(f'Training stopped after {len(training_history[\"train_losses\"])} epochs')\n",
    "\n",
    "# Show genre preferences by country (for classification)\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"PREDICTIONS BY COUNTRY\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Create input for each country \n",
    "country_inputs = np.eye(len(country_encoder.classes_))\n",
    "genre_probs = nn.predict_proba(country_inputs)  # Use predict_proba for cleaner interface\n",
    "\n",
    "for i, country in enumerate(country_encoder.classes_):\n",
    "    predicted_genre_idx = np.argmax(genre_probs[i])\n",
    "    predicted_genre = genre_encoder.classes_[predicted_genre_idx]\n",
    "    confidence = genre_probs[i][predicted_genre_idx]\n",
    "\n",
    "    print(f\"{country}: {predicted_genre} ({confidence:.2%} confidence)\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
