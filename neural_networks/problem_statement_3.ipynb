{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf7dd43a",
   "metadata": {},
   "source": [
    "# The Problem: What are by age what are the preferred listening times of users (Morning/Afternoon/Night)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b4d69914",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48396702",
   "metadata": {},
   "source": [
    "## Load Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "581857b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "music_data = pd.read_csv('../data/clean_data/age_time_of_day.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c86101b",
   "metadata": {},
   "source": [
    "## Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd6fa706",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset Overview\n",
      "Total users: 5000\n",
      "Age range: 13 - 60 years\n",
      "Average age: 36.7 years\n"
     ]
    }
   ],
   "source": [
    "age_feature = 'age'\n",
    "target_feature = 'listening_time'\n",
    "\n",
    "x_raw = music_data[age_feature].values.reshape(-1,1)\n",
    "y_raw = music_data[target_feature].values\n",
    "\n",
    "print(f\"\\nDataset Overview\")\n",
    "print(f\"Total users: {len(music_data)}\")\n",
    "print(f\"Age range: {x_raw.min():.0f} - {x_raw.max():.0f} years\")\n",
    "print(f\"Average age: {x_raw.mean():.1f} years\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ba9da72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Listening Time Distributions:\n",
      "Afternoon: [1634 1621 1745] users (32.7%)\n",
      "Morning: [1634 1621 1745] users (32.4%)\n",
      "Night: [1634 1621 1745] users (34.9%)\n",
      "\n",
      "Class balance ratio: 1.08:1\n",
      "classes are well balanced\n",
      "\n",
      "Target encoding\n",
      "Afternoon -> 0\n",
      "Morning -> 1\n",
      "Night -> 2\n"
     ]
    }
   ],
   "source": [
    "# Analyze target distribution \n",
    "unique_times, counts = np.unique(y_raw,return_counts=True)\n",
    "print(f\"\\nListening Time Distributions:\")\n",
    "for time, count in zip(unique_times, counts):\n",
    "    percentage = count / len(y_raw) * 100\n",
    "    print(f\"{time}: {counts} users ({percentage:.1f}%)\")\n",
    "\n",
    "# Check for class imbalance\n",
    "max_count, min_count = counts.max(), counts.min()\n",
    "imbalance_ratio = max_count / min_count\n",
    "print(f\"\\nClass balance ratio: {imbalance_ratio:.2f}:1\")\n",
    "if imbalance_ratio > 2:\n",
    "    print(\"Moderate class imbalance detected\")\n",
    "else: \n",
    "    print('classes are well balanced')\n",
    "    # Encode target labels to integers\n",
    "    label_encoder = LabelEncoder()\n",
    "    y_encoded = label_encoder.fit_transform(y_raw)\n",
    "\n",
    "    print(f\"\\nTarget encoding\")\n",
    "    for i, class_name in enumerate(label_encoder.classes_):\n",
    "        print(f\"{class_name} -> {i}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "607d9696",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Feature preprocessing:\n",
      "Original age range: 13 - 60\n",
      "Scaled age range: -1.72 - 1.70\n",
      "Age mean after scaling: -0.000\n",
      "Age std after scaling: 1.000\n",
      "\n",
      "Data shapes:\n",
      "Input features (X): (5000, 1)\n",
      "Output labels (y): (5000, 3)\n"
     ]
    }
   ],
   "source": [
    "# one-hot encode targets for neural network\n",
    "def one_hot_encode(data,num_classes):\n",
    "    \"\"\"Convert integer labels to one-hot vectors\"\"\"\n",
    "    encoded = np.zeros((len(data),num_classes))\n",
    "    encoded[np.arange(len(data)),data] = 1\n",
    "    return encoded\n",
    "\n",
    "y_onehot = one_hot_encode(y_encoded, len(label_encoder.classes_))\n",
    "\n",
    "## Normalize age feature \n",
    "scaler = StandardScaler()\n",
    "x_scaled = scaler.fit_transform(x_raw)\n",
    "\n",
    "print(f\"\\nFeature preprocessing:\")\n",
    "print(f\"Original age range: {x_raw.min():.0f} - {x_raw.max():.0f}\")\n",
    "print(f\"Scaled age range: {x_scaled.min():.2f} - {x_scaled.max():.2f}\")\n",
    "print(f\"Age mean after scaling: {x_scaled.mean():.3f}\")\n",
    "print(f\"Age std after scaling: {x_scaled.std():.3f}\")\n",
    "\n",
    "print(f\"\\nData shapes:\")\n",
    "print(f\"Input features (X): {x_scaled.shape}\")\n",
    "print(f\"Output labels (y): {y_onehot.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "705f0c9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stratified data splits:\n",
      "Training set: 3000 users\n",
      "Validation set: 1000 users\n",
      "Test set: 1000 users\n"
     ]
    }
   ],
   "source": [
    "# Split data into train/val/test (60%/20%/20%)\n",
    "x_train, x_temp, y_train, y_temp = train_test_split(x_scaled,y_onehot, test_size=0.4, random_state=42)\n",
    "x_val,x_test,y_val,y_test = train_test_split(x_temp,y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "print(f\"\\nStratified data splits:\")\n",
    "print(f\"Training set: {x_train.shape[0]} users\")\n",
    "print(f\"Validation set: {x_val.shape[0]} users\")\n",
    "print(f\"Test set: {x_test.shape[0]} users\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "eea51b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creation of the Neural Network class \n",
    "class ClassificationNeuralNetwork:\n",
    "    \"\"\"\n",
    "    2-layer Neural Network specialized for Classification tasks\n",
    "    - Multi-class output with softmax activation\n",
    "    - Cross-entropy loss function\n",
    "    - Accuracy evaluation metric\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        \"\"\"\n",
    "        Initialize classification neural network\n",
    "        \n",
    "        Args:\n",
    "            input_size: Number of input features\n",
    "            hidden_size: Number of hidden layer neurons\n",
    "            output_size: Number of classes to predict\n",
    "        \"\"\"\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        # Xavier initialization for stable training\n",
    "        self.W1 = np.random.randn(input_size, hidden_size) / np.sqrt(input_size)\n",
    "        self.b1 = np.zeros((1, hidden_size))\n",
    "        \n",
    "        self.W2 = np.random.randn(hidden_size, output_size) / np.sqrt(hidden_size)\n",
    "        self.b2 = np.zeros((1, output_size))\n",
    "        \n",
    "    def relu(self, x):\n",
    "        \"\"\"ReLU activation: f(x) = max(0, x)\"\"\"\n",
    "        return np.maximum(0, x)\n",
    "    \n",
    "    def relu_derivative(self, x):\n",
    "        \"\"\"ReLU derivative: f'(x) = 1 if x > 0, else 0\"\"\"\n",
    "        return (x > 0).astype(float)\n",
    "    \n",
    "    def softmax(self, x):\n",
    "        \"\"\"\n",
    "        Softmax activation for multi-class classification\n",
    "        Converts logits to probability distribution\n",
    "        \"\"\"\n",
    "        exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "        return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "    \n",
    "    def forward_pass(self, X):\n",
    "        \"\"\"\n",
    "        Forward propagation for classification:\n",
    "        z1 = X @ W1 + b1\n",
    "        a1 = ReLU(z1)\n",
    "        z2 = a1 @ W2 + b2\n",
    "        a2 = softmax(z2)  # Probability distribution\n",
    "        \"\"\"\n",
    "        # Hidden layer\n",
    "        self.z1 = X @ self.W1 + self.b1\n",
    "        self.a1 = self.relu(self.z1)\n",
    "        \n",
    "        # Output layer with softmax\n",
    "        self.z2 = self.a1 @ self.W2 + self.b2\n",
    "        self.a2 = self.softmax(self.z2)\n",
    "        \n",
    "        return self.a2\n",
    "    \n",
    "    def cross_entropy_loss(self, y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Cross-entropy loss for multi-class classification:\n",
    "        L = -Î£(y_true * log(y_pred)) / m\n",
    "        \"\"\"\n",
    "        epsilon = 1e-15  # Prevent log(0)\n",
    "        y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "        \n",
    "        m = y_true.shape[0]\n",
    "        loss = -np.sum(y_true * np.log(y_pred)) / m\n",
    "        return loss\n",
    "    \n",
    "    def backward_pass(self, X, y_true):\n",
    "        \"\"\"\n",
    "        Backpropagation for classification:\n",
    "        \n",
    "        Output layer (softmax + cross-entropy):\n",
    "        dL/dz2 = a2 - y_true\n",
    "        \n",
    "        Hidden layer:\n",
    "        dL/da1 = dL/dz2 @ W2^T\n",
    "        dL/dz1 = dL/da1 * ReLU'(z1)\n",
    "        \"\"\"\n",
    "        m = X.shape[0]\n",
    "        \n",
    "        # Output layer gradients (softmax + cross-entropy derivative)\n",
    "        dz2 = self.a2 - y_true\n",
    "        dW2 = self.a1.T @ dz2 / m\n",
    "        db2 = np.sum(dz2, axis=0, keepdims=True) / m\n",
    "        \n",
    "        # Hidden layer gradients\n",
    "        da1 = dz2 @ self.W2.T\n",
    "        dz1 = da1 * self.relu_derivative(self.z1)\n",
    "        dW1 = X.T @ dz1 / m\n",
    "        db1 = np.sum(dz1, axis=0, keepdims=True) / m\n",
    "        \n",
    "        return dW1, db1, dW2, db2\n",
    "    \n",
    "    def train(self, X_train, y_train, X_val, y_val, epochs=1000, learning_rate=0.01, early_stopping_patience=50):\n",
    "        \"\"\"Train classification neural network with validation monitoring\"\"\"\n",
    "        train_losses = []\n",
    "        val_losses = []\n",
    "        val_accuracies = []\n",
    "        \n",
    "        best_val_loss = float('inf')\n",
    "        patience_counter = 0\n",
    "        best_weights = None\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            # Forward pass and training loss\n",
    "            y_pred_train = self.forward_pass(X_train)\n",
    "            train_loss = self.cross_entropy_loss(y_train, y_pred_train)\n",
    "            train_losses.append(train_loss)\n",
    "            \n",
    "            # Backward pass and weight updates\n",
    "            dW1, db1, dW2, db2 = self.backward_pass(X_train, y_train)\n",
    "            \n",
    "            self.W1 -= learning_rate * dW1\n",
    "            self.b1 -= learning_rate * db1\n",
    "            self.W2 -= learning_rate * dW2\n",
    "            self.b2 -= learning_rate * db2\n",
    "            \n",
    "            # Validation evaluation\n",
    "            y_pred_val = self.forward_pass(X_val)\n",
    "            val_loss = self.cross_entropy_loss(y_val, y_pred_val)\n",
    "            val_losses.append(val_loss)\n",
    "            \n",
    "            val_accuracy = self.accuracy(X_val, y_val)\n",
    "            val_accuracies.append(val_accuracy)\n",
    "            \n",
    "            # Early stopping logic\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                patience_counter = 0\n",
    "                best_weights = {\n",
    "                    'W1': self.W1.copy(), 'b1': self.b1.copy(),\n",
    "                    'W2': self.W2.copy(), 'b2': self.b2.copy()\n",
    "                }\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "            \n",
    "            if epoch % 100 == 0:\n",
    "                print(f\"Epoch {epoch} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | Val Acc: {val_accuracy:.4f}\")\n",
    "            \n",
    "            if patience_counter >= early_stopping_patience:\n",
    "                print(f\"Early stopping at epoch {epoch}. Best validation loss: {best_val_loss:.4f}\")\n",
    "                self.W1, self.b1 = best_weights['W1'], best_weights['b1']\n",
    "                self.W2, self.b2 = best_weights['W2'], best_weights['b2']\n",
    "                break\n",
    "        \n",
    "        return {\n",
    "            'train_losses': train_losses,\n",
    "            'val_losses': val_losses,\n",
    "            'val_accuracies': val_accuracies\n",
    "        }\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Get class predictions (argmax of probabilities)\"\"\"\n",
    "        probabilities = self.forward_pass(X)\n",
    "        return np.argmax(probabilities, axis=1)\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Get prediction probabilities\"\"\"\n",
    "        return self.forward_pass(X)\n",
    "    \n",
    "    def accuracy(self, X, y_true):\n",
    "        \"\"\"Calculate classification accuracy\"\"\"\n",
    "        predictions = self.predict(X)\n",
    "        y_true_classes = np.argmax(y_true, axis=1)\n",
    "        return np.mean(predictions == y_true_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "08c7fdd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Architecture: 1 -> 10 -> 3\n",
      "Input: Age (normalized)\n",
      "Output: 3 listening time categories\n",
      "Total parameters: 53\n",
      "Data points per parameter: 94.3\n"
     ]
    }
   ],
   "source": [
    "# Network architecture\n",
    "input_size = x_scaled.shape[1]    # 1 (age)\n",
    "hidden_size = 10                  # Hidden layer neurons\n",
    "output_size = len(label_encoder.classes_)  # 3 (Morning/Afternoon/Night)\n",
    "\n",
    "print(f\"Architecture: {input_size} -> {hidden_size} -> {output_size}\")\n",
    "print(f\"Input: Age (normalized)\")\n",
    "print(f\"Output: {len(label_encoder.classes_)} listening time categories\")\n",
    "\n",
    "# Calculate parameters\n",
    "total_params = input_size * hidden_size + hidden_size * output_size + hidden_size + output_size\n",
    "print(f\"Total parameters: {total_params}\")\n",
    "print(f\"Data points per parameter: {len(x_scaled) / total_params:.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dbe5e09d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 | Train Loss: 1.1412 | Val Loss: 1.1421 | Val Acc: 0.3350\n",
      "Epoch 100 | Train Loss: 1.0983 | Val Loss: 1.0992 | Val Acc: 0.3300\n",
      "Epoch 200 | Train Loss: 1.0982 | Val Loss: 1.0989 | Val Acc: 0.3460\n",
      "Epoch 300 | Train Loss: 1.0981 | Val Loss: 1.0988 | Val Acc: 0.3460\n",
      "Epoch 400 | Train Loss: 1.0981 | Val Loss: 1.0988 | Val Acc: 0.3470\n",
      "Epoch 500 | Train Loss: 1.0981 | Val Loss: 1.0988 | Val Acc: 0.3470\n",
      "Epoch 600 | Train Loss: 1.0981 | Val Loss: 1.0989 | Val Acc: 0.3450\n",
      "Early stopping at epoch 664. Best validation loss: 1.0988\n",
      "\n",
      "FINAL RESULTS:\n",
      "Training Accuracy: 0.3447\n",
      "Validation Accuracy: 0.3470\n",
      "Test Accuracy: 0.3660\n",
      "Random baseline: 0.3333 (33.3%)\n"
     ]
    }
   ],
   "source": [
    "# Initialize network\n",
    "nn = ClassificationNeuralNetwork(input_size, hidden_size, output_size)\n",
    "\n",
    "# Train the network\n",
    "training_history = nn.train(x_train, y_train, x_val, y_val, epochs=2000, learning_rate=0.1, early_stopping_patience=200)\n",
    "\n",
    "\n",
    "# Evaluate performance on all three sets\n",
    "train_accuracy = nn.accuracy(x_train, y_train)\n",
    "val_accuracy = nn.accuracy(x_val, y_val)\n",
    "test_accuracy = nn.accuracy(x_test, y_test)\n",
    "\n",
    "print(f\"\\nFINAL RESULTS:\")\n",
    "print(f\"Training Accuracy: {train_accuracy:.4f}\")\n",
    "print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "# Calculate baseline accuracy (random guessing)\n",
    "random_baseline = 1.0 / len(label_encoder.classes_)\n",
    "print(f\"Random baseline: {random_baseline:.4f} ({random_baseline*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26252ee",
   "metadata": {},
   "source": [
    "# This model actual performed the best out of all the other problems.\n",
    "The model shows that it is actually learning:\n",
    "Current classification: 36.6% vs 33.3% random = 10% relative improvement!\n",
    "\n",
    "There is good generalization meaning there is no overfitting with the test performing the best:\n",
    "Training:   34.5%\n",
    "Validation: 34.7%  \n",
    "Test:       36.6%   <- Best performance!\n",
    "\n",
    "The learning is also stable showing consistent, and steady improvement: \n",
    "Epoch 0:   33.5% (random level)\n",
    "Epoch 664: 34.7% (learned patterns)\n",
    "\n",
    "However it is not perfect:\n",
    "- Only 3.3 percentage points above random\n",
    "    * Could be that age alone does not strongly predict listening preferences\n",
    "- Learning plateaus\n",
    "    * Epoch 200-600: Accuracy stuck around 34.7%\n",
    "    * It was able to find weak patterns quickly, then it plateaued\n",
    "- Minimal Loss Reduction:\n",
    "    * Loss: 1.14 -> 1.10 \n",
    "    * The small decrease suggests limited learnable signal in the data\n",
    "\n",
    "I am satisfied with these results because it shows:\n",
    "- Age has some influence on listening time preferences \n",
    "- But age alone is not a strong predictor\n",
    "- The relationship exists but is weak\n",
    "\n",
    "What I could do is go back and add in some more features to see if there are other correlations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6103d9c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model shows good generalization\n",
      "\n",
      "============================================================\n",
      "LISTENING TIME PREDICTIONS BY AGE\n",
      "============================================================\n",
      "Analyzing listening preferences across age spectrum:\n",
      "\n",
      "Age   Morning    Afternoon  Night      Predicted   \n",
      "-------------------------------------------------------\n",
      "18    0.318      0.338      0.344      Night       \n",
      "23    0.325      0.334      0.341      Night       \n",
      "28    0.332      0.329      0.339      Night       \n",
      "33    0.340      0.324      0.336      Morning     \n",
      "38    0.330      0.319      0.351      Night       \n",
      "43    0.330      0.320      0.350      Night       \n",
      "48    0.329      0.322      0.349      Night       \n",
      "53    0.329      0.324      0.348      Night       \n",
      "58    0.328      0.325      0.347      Night       \n"
     ]
    }
   ],
   "source": [
    "# Check for overfitting\n",
    "if train_accuracy - test_accuracy > 0.1:\n",
    "    print(\"Model may be overfitting (train-test gap > 10%)\")\n",
    "else:\n",
    "    print(\"Model shows good generalization\")\n",
    "\n",
    "# Show listening time predictions across age spectrum\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"LISTENING TIME PREDICTIONS BY AGE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "age_range = np.arange(18, 61, 5)  # Ages 18, 23, 28, ..., 58\n",
    "print(f\"Analyzing listening preferences across age spectrum:\")\n",
    "\n",
    "print(f\"\\n{'Age':<5} {'Morning':<10} {'Afternoon':<10} {'Night':<10} {'Predicted':<12}\")\n",
    "print(\"-\" * 55)\n",
    "\n",
    "for age in age_range:\n",
    "    # Normalize age using same scaler\n",
    "    age_normalized = scaler.transform([[age]])\n",
    "    \n",
    "    # Get predictions\n",
    "    probabilities = nn.predict_proba(age_normalized)[0]\n",
    "    predicted_class = np.argmax(probabilities)\n",
    "    predicted_time = label_encoder.classes_[predicted_class]\n",
    "    \n",
    "    # Display probabilities and prediction\n",
    "    morning_prob = probabilities[0] if label_encoder.classes_[0] == 'Morning' else probabilities[np.where(label_encoder.classes_ == 'Morning')[0][0]]\n",
    "    afternoon_prob = probabilities[1] if label_encoder.classes_[1] == 'Afternoon' else probabilities[np.where(label_encoder.classes_ == 'Afternoon')[0][0]]\n",
    "    night_prob = probabilities[2] if label_encoder.classes_[2] == 'Night' else probabilities[np.where(label_encoder.classes_ == 'Night')[0][0]]\n",
    "    \n",
    "    print(f\"{age:<5} {morning_prob:<10.3f} {afternoon_prob:<10.3f} {night_prob:<10.3f} {predicted_time:<12}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
